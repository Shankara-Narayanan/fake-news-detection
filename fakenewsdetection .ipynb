{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49172de2",
   "metadata": {},
   "source": [
    "### data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c709a2c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import seaborn as sb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56393ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_filename = 'test.csv'\n",
    "train_filename = 'train.csv'\n",
    "valid_filename = 'valid.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "16360f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_news = pd.read_csv(train_filename)\n",
    "test_news = pd.read_csv(test_filename)\n",
    "valid_news = pd.read_csv(valid_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3748165c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data observation\n",
    "def data_obs():\n",
    "    print(\"training dataset size:\")\n",
    "    print(train_news.shape)\n",
    "    print(train_news.head(10))\n",
    "\n",
    "    #below dataset were used for testing and validation purposes\n",
    "    print(test_news.shape)\n",
    "    print(test_news.head(10))\n",
    "    \n",
    "    print(valid_news.shape)\n",
    "    print(valid_news.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7eaab31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check the data by calling below function\n",
    "#data_obs()\n",
    "\n",
    "#distribution of classes for prediction\n",
    "def create_distribution(dataFile):\n",
    "    \n",
    "    return sb.countplot(x='Label', data=dataFile, palette='hls')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "775da2a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='Label', ylabel='count'>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#by calling below we can see that training, test and valid data seems to be failry evenly distributed between the classes\n",
    "create_distribution(train_news)\n",
    "create_distribution(test_news)\n",
    "create_distribution(valid_news)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3a69ec8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data integrity check (missing label values)\n",
    "#none of the datasets contains missing values therefore no cleaning required\n",
    "def data_qualityCheck():\n",
    "    \n",
    "    print(\"Checking data qualitites...\")\n",
    "    train_news.isnull().sum()\n",
    "    train_news.info()\n",
    "        \n",
    "    print(\"check finished.\")\n",
    "\n",
    "    #below datasets were used to \n",
    "    test_news.isnull().sum()\n",
    "    test_news.info()\n",
    "\n",
    "    valid_news.isnull().sum()\n",
    "    valid_news.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a9afc05",
   "metadata": {},
   "outputs": [],
   "source": [
    "#run the below function call to see the quality check results\n",
    "#data_qualityCheck()\n",
    "\n",
    "\n",
    "\n",
    "#eng_stemmer = SnowballStemmer('english')\n",
    "#stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "#Stemming\n",
    "def stem_tokens(tokens, stemmer):\n",
    "    stemmed = []\n",
    "    for token in tokens:\n",
    "        stemmed.append(stemmer.stem(token))\n",
    "    return stemmed\n",
    "\n",
    "#process the data\n",
    "def process_data(data,exclude_stopword=True,stem=True):\n",
    "    tokens = [w.lower() for w in data]\n",
    "    tokens_stemmed = tokens\n",
    "    tokens_stemmed = stem_tokens(tokens, eng_stemmer)\n",
    "    tokens_stemmed = [w for w in tokens_stemmed if w not in stopwords ]\n",
    "    return tokens_stemmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6b557e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating ngrams\n",
    "#unigram \n",
    "def create_unigram(words):\n",
    "    assert type(words) == list\n",
    "    return words\n",
    "\n",
    "#bigram\n",
    "def create_bigrams(words):\n",
    "    assert type(words) == list\n",
    "    skip = 0\n",
    "    join_str = \" \"\n",
    "    Len = len(words)\n",
    "    if Len > 1:\n",
    "        lst = []\n",
    "        for i in range(Len-1):\n",
    "            for k in range(1,skip+2):\n",
    "                if i+k < Len:\n",
    "                    lst.append(join_str.join([words[i],words[i+k]]))\n",
    "    else:\n",
    "        #set it as unigram\n",
    "        lst = create_unigram(words)\n",
    "    return lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ef99c6ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n#trigrams\\ndef create_trigrams(words):\\n    assert type(words) == list\\n    skip == 0\\n    join_str = \" \"\\n    Len = len(words)\\n    if L > 2:\\n        lst = []\\n        for i in range(1,skip+2):\\n            for k1 in range(1, skip+2):\\n                for k2 in range(1,skip+2):\\n                    for i+k1 < Len and i+k1+k2 < Len:\\n                        lst.append(join_str.join([words[i], words[i+k1],words[i+k1+k2])])\\n        else:\\n            #set is as bigram\\n            lst = create_bigram(words)\\n    return lst\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "#trigrams\n",
    "def create_trigrams(words):\n",
    "    assert type(words) == list\n",
    "    skip == 0\n",
    "    join_str = \" \"\n",
    "    Len = len(words)\n",
    "    if L > 2:\n",
    "        lst = []\n",
    "        for i in range(1,skip+2):\n",
    "            for k1 in range(1, skip+2):\n",
    "                for k2 in range(1,skip+2):\n",
    "                    for i+k1 < Len and i+k1+k2 < Len:\n",
    "                        lst.append(join_str.join([words[i], words[i+k1],words[i+k1+k2])])\n",
    "        else:\n",
    "            #set is as bigram\n",
    "            lst = create_bigram(words)\n",
    "    return lst\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d61ff41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "porter = PorterStemmer()\n",
    "\n",
    "def tokenizer(text):\n",
    "    return text.split()\n",
    "\n",
    "\n",
    "def tokenizer_porter(text):\n",
    "    return [porter.stem(word) for word in text.split()]\n",
    "\n",
    "#doc = ['runners like running and thus they run','this is a test for tokens']\n",
    "#tokenizer([word for line in test_news.iloc[:,1] for word in line.lower().split()])\n",
    "\n",
    "#show the distribution of labels in the train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "37040885",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'#converting multiclass labels present in our datasets to binary class labels\\nfor i , row in data_TrainNews.iterrows():\\n    if (data_TrainNews.iloc[:,0] == \"mostly-true\" | data_TrainNews.iloc[:,0] == \"half-true\" | data_TrainNews.iloc[:,0] == \"true\"):\\n        data_TrainNews.iloc[:,0] = \"true\"\\n    else :\\n        data_TrainNews.iloc[:,0] = \"false\"\\n        \\nfor i,row in data_TrainNews.iterrows():\\n    print(row)\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"def create_datafile(filename)\n",
    "    #function to slice the dataframe to keep variables necessary to be used for classification\n",
    "    return \"return df to be used\"\n",
    "\"\"\"\n",
    "    \n",
    "\"\"\"#converting multiclass labels present in our datasets to binary class labels\n",
    "for i , row in data_TrainNews.iterrows():\n",
    "    if (data_TrainNews.iloc[:,0] == \"mostly-true\" | data_TrainNews.iloc[:,0] == \"half-true\" | data_TrainNews.iloc[:,0] == \"true\"):\n",
    "        data_TrainNews.iloc[:,0] = \"true\"\n",
    "    else :\n",
    "        data_TrainNews.iloc[:,0] = \"false\"\n",
    "        \n",
    "for i,row in data_TrainNews.iterrows():\n",
    "    print(row)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c9a8d34",
   "metadata": {},
   "source": [
    "### feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b78557c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in c:\\users\\admin\\anaconda3\\lib\\site-packages (4.3.2)\n",
      "Requirement already satisfied: numpy>=1.18.5 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from gensim) (1.22.0)\n",
      "Requirement already satisfied: scipy>=1.7.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from gensim) (1.7.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from gensim) (7.0.1)\n",
      "Requirement already satisfied: wrapt in c:\\users\\admin\\anaconda3\\lib\\site-packages (from smart-open>=1.8.1->gensim) (1.12.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0d9ba114",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEJCAYAAABlmAtYAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAATuklEQVR4nO3df5Bd5X3f8fcnwgZSTAxhoSBBRFLFDRCDy46GxJ0ONa5RJz8gjsmICUVNSOUwNHEydR3odFoyGU2YaerE0EDMNDEicU0UJxTFNXEVxTT1BIOXGgcLzKAxGGRhJOO2hgyhQfn2j/swvl5d7XOFde/uat+vmTPn3O85z7nP6oI+Os9z7tlUFZIkLeTbFrsDkqSlz7CQJHUZFpKkLsNCktRlWEiSugwLSVLXRMMiyRuTfDTJF5I8luQHkpycZEeSJ9r6pKHjb0iyO8njSS4dql+Y5JG27+YkmWS/JUnfbNJXFh8A/qSq/j5wPvAYcD2ws6rWATvba5KcA2wEzgU2ALcmWdXOcxuwGVjXlg0T7rckaUgm9aW8JCcCnwO+u4beJMnjwMVV9WyS04H7qupNSW4AqKpfbcd9ArgReAr4ZAscklzZ2r97ofc/5ZRTau3atUf855Kko9lDDz301aqamV8/ZoLv+d3AfuBDSc4HHgLeA5xWVc8CtMA4tR2/Gvj0UPs9rfY3bXt+fUFr165lbm7uW/4hJGklSfKlUfVJDkMdA/wD4LaqegvwV7Qhp0MYNQ9RC9QPPkGyOclckrn9+/cfbn8lSYcwybDYA+ypqgfa648yCI/n2vATbb1v6Pgzh9qvAfa2+poR9YNU1e1VNVtVszMzB11FSZJeo4mFRVV9BXgmyZta6RLgUWA7sKnVNgH3tO3twMYkxyY5m8FE9oNtyOqFJBe1u6CuHmojSZqCSc5ZAPwc8OEkrwe+CPwUg4DaluQa4GngCoCq2pVkG4NAeQW4rqoOtPNcC9wBHA/c2xZJ0pRM7G6oxTY7O1tOcEvS4UnyUFXNzq/7DW5JUpdhIUnqMiwkSV2GhSSpa9J3Q0kT9bN/4U0M0/BbP3jQfKdWGK8sJEldhoUkqcuwkCR1GRaSpC7DQpLUZVhIkroMC0lSl2EhSeoyLCRJXYaFJKnLsJAkdRkWkqQuw0KS1GVYSJK6DAtJUpdhIUnqMiwkSV2GhSSpy7CQJHUZFpKkLsNCktRlWEiSuiYaFkmeSvJIkoeTzLXayUl2JHmirU8aOv6GJLuTPJ7k0qH6he08u5PcnCST7Lck6ZtN48riH1fVBVU1215fD+ysqnXAzvaaJOcAG4FzgQ3ArUlWtTa3AZuBdW3ZMIV+S5KaxRiGugzY2ra3ApcP1e+qqper6klgN7A+yenAiVV1f1UVcOdQG0nSFEw6LAr470keSrK51U6rqmcB2vrUVl8NPDPUdk+rrW7b8+uSpCk5ZsLnf2tV7U1yKrAjyRcWOHbUPEQtUD/4BINA2gxw1llnHW5fJUmHMNEri6ra29b7gLuB9cBzbWiJtt7XDt8DnDnUfA2wt9XXjKiPer/bq2q2qmZnZmaO5I8iSSvaxMIiyd9J8oZXt4F3AJ8HtgOb2mGbgHva9nZgY5Jjk5zNYCL7wTZU9UKSi9pdUFcPtZEkTcEkh6FOA+5ud7keA/yXqvqTJJ8BtiW5BngauAKgqnYl2QY8CrwCXFdVB9q5rgXuAI4H7m2LJGlKJhYWVfVF4PwR9eeBSw7RZguwZUR9DjjvSPdRkjQev8EtSeoyLCRJXYaFJKnLsJAkdRkWkqQuw0KS1GVYSJK6DAtJUpdhIUnqMiwkSV2GhSSpy7CQJHUZFpKkrkn/prxlYe7nf3axu3DUm735txa7C5K+BV5ZSJK6DAtJUpdhIUnqMiwkSV2GhSSpy7CQJHUZFpKkLsNCktRlWEiSugwLSVKXYSFJ6jIsJEldhoUkqWviYZFkVZLPJvlYe31ykh1Jnmjrk4aOvSHJ7iSPJ7l0qH5hkkfavpuTZNL9liR9wzSuLN4DPDb0+npgZ1WtA3a21yQ5B9gInAtsAG5Nsqq1uQ3YDKxry4Yp9FuS1Ew0LJKsAX4I+M9D5cuArW17K3D5UP2uqnq5qp4EdgPrk5wOnFhV91dVAXcOtZEkTcGkryx+A3gf8LdDtdOq6lmAtj611VcDzwwdt6fVVrft+XVJ0pRMLCyS/DCwr6oeGrfJiFotUB/1npuTzCWZ279//5hvK0nqmeSVxVuBH03yFHAX8LYkvwc814aWaOt97fg9wJlD7dcAe1t9zYj6Qarq9qqararZmZmZI/mzSNKKNrGwqKobqmpNVa1lMHH9Z1V1FbAd2NQO2wTc07a3AxuTHJvkbAYT2Q+2oaoXklzU7oK6eqiNJGkKjlmE97wJ2JbkGuBp4AqAqtqVZBvwKPAKcF1VHWhtrgXuAI4H7m2LJGlKphIWVXUfcF/bfh645BDHbQG2jKjPAedNroeSpIX4DW5JUpdhIUnqMiwkSV2GhSSpy7CQJHUZFpKkLsNCktRlWEiSugwLSVKXYSFJ6jIsJEldhoUkqcuwkCR1GRaSpC7DQpLUZVhIkroMC0lSl2EhSeoyLCRJXWOFRZKd49QkSUenYxbameQ44NuBU5KcBKTtOhE4Y8J9kyQtEQuGBfBu4BcYBMNDfCMsvg785uS6JUlaShYMi6r6APCBJD9XVbdMqU+SpCWmd2UBQFXdkuQHgbXDbarqzgn1S5K0hIwVFkl+F/ge4GHgQCsXYFhI0gowVlgAs8A5VVWT7IwkaWka93sWnwf+7iQ7IklausYNi1OAR5N8Isn2V5eFGiQ5LsmDST6XZFeSX271k5PsSPJEW5801OaGJLuTPJ7k0qH6hUkeaftuTpJR7ylJmoxxh6FufA3nfhl4W1W9mOR1wKeS3Au8E9hZVTcluR64HvilJOcAG4FzGdyq+6dJvreqDgC3AZuBTwMfBzYA976GPkmSXoNx74b6H4d74ja/8WJ7+bq2FHAZcHGrbwXuA36p1e+qqpeBJ5PsBtYneQo4saruB0hyJ3A5hoUkTc24j/t4IcnX2/LXSQ4k+foY7VYleRjYB+yoqgeA06rqWYC2PrUdvhp4Zqj5nlZb3bbn1yVJUzLulcUbhl8nuRxYP0a7A8AFSd4I3J3kvAUOHzUPUQvUDz5BspnBcBVnnXVWr3uSpDG9pqfOVtV/Bd52GMf/HwbDTRuA55KcDtDW+9phe4Azh5qtAfa2+poR9VHvc3tVzVbV7MzMzLjdkyR1jDsM9c6h5V1JbuIQ/7ofajPTrihIcjzwduALwHZgUztsE3BP294ObExybJKzgXXAg22o6oUkF7W7oK4eaiNJmoJx74b6kaHtV4CnGExIL+R0YGuSVQxCaVtVfSzJ/cC2JNcATwNXAFTVriTbgEfbe1zXhrEArgXuAI5nMLHt5LYkTdG4cxY/dbgnrqq/BN4yov48cMkh2mwBtoyozwELzXdIkiZo3GGoNUnuTrIvyXNJ/jDJmn5LSdLRYNwJ7g8xmFM4g8Ftq3/capKkFWDcsJipqg9V1SttuQPwdiNJWiHGDYuvJrmqfcluVZKrgOcn2TFJ0tIxblj8NPATwFeAZ4F3AYc96S1JWp7GvXX2V4BNVfW/YfDkWODXGISIJOkoN+6VxZtfDQqAqvoaI26LlSQdncYNi2+b93snTmb8qxJJ0jI37l/4/xH4iyQfZfCYj59gxJfnJElHp3G/wX1nkjkGDw8M8M6qenSiPZMkLRljDyW1cDAgJGkFek2PKJckrSyGhSSpy7CQJHUZFpKkLsNCktRlWEiSugwLSVKXYSFJ6jIsJEldhoUkqcuwkCR1GRaSpC7DQpLUZVhIkroMC0lSl2EhSeqaWFgkOTPJJ5M8lmRXkve0+slJdiR5oq2Hf7f3DUl2J3k8yaVD9QuTPNL23Zwkk+q3JOlgk7yyeAX4V1X1fcBFwHVJzgGuB3ZW1TpgZ3tN27cROBfYANyaZFU7123AZmBdWzZMsN+SpHkmFhZV9WxV/a+2/QLwGLAauAzY2g7bClzeti8D7qqql6vqSWA3sD7J6cCJVXV/VRVw51AbSdIUTGXOIsla4C3AA8BpVfUsDAIFOLUdthp4ZqjZnlZb3bbn1yVJUzLxsEhyAvCHwC9U1dcXOnRErRaoj3qvzUnmkszt37//8DsrSRppomGR5HUMguLDVfVHrfxcG1qirfe1+h7gzKHma4C9rb5mRP0gVXV7Vc1W1ezMzMyR+0EkaYWb5N1QAX4beKyq3j+0azuwqW1vAu4Zqm9McmySsxlMZD/YhqpeSHJRO+fVQ20kSVNwzATP/VbgnwGPJHm41f4NcBOwLck1wNPAFQBVtSvJNuBRBndSXVdVB1q7a4E7gOOBe9siSZqSiYVFVX2K0fMNAJccos0WYMuI+hxw3pHrnSTpcPgNbklSl2EhSeoyLCRJXYaFJKnLsJAkdRkWkqQuw0KS1GVYSJK6DAtJUpdhIUnqMiwkSV2GhSSpy7CQJHUZFpKkLsNCktRlWEiSugwLSVKXYSFJ6jIsJEldE/sd3NJUvP7Oxe7BCjG72B3QIvPKQpLUZVhIkroMC0lSl2EhSeoyLCRJXYaFJKnLsJAkdU0sLJL8TpJ9ST4/VDs5yY4kT7T1SUP7bkiyO8njSS4dql+Y5JG27+YkmVSfJUmjTfLK4g5gw7za9cDOqloH7GyvSXIOsBE4t7W5Ncmq1uY2YDOwri3zzylJmrCJhUVV/TnwtXnly4CtbXsrcPlQ/a6qermqngR2A+uTnA6cWFX3V1UBdw61kSRNybTnLE6rqmcB2vrUVl8NPDN03J5WW92259clSVO0VCa4R81D1AL10SdJNieZSzK3f//+I9Y5SVrpph0Wz7WhJdp6X6vvAc4cOm4NsLfV14yoj1RVt1fVbFXNzszMHNGOS9JKNu2w2A5satubgHuG6huTHJvkbAYT2Q+2oaoXklzU7oK6eqiNJGlKJvaI8iQfAS4GTkmyB/j3wE3AtiTXAE8DVwBU1a4k24BHgVeA66rqQDvVtQzurDoeuLctkqQpmlhYVNWVh9h1ySGO3wJsGVGfA847gl2TJB2mpTLBLUlawgwLSVKXYSFJ6jIsJEldhoUkqcuwkCR1GRaSpC7DQpLUZVhIkroMC0lSl2EhSeoyLCRJXYaFJKnLsJAkdRkWkqQuw0KS1GVYSJK6DAtJUpdhIUnqMiwkSV2GhSSpy7CQJHUZFpKkLsNCktRlWEiSugwLSVKXYSFJ6lo2YZFkQ5LHk+xOcv1i90eSVpJlERZJVgG/CfxT4BzgyiTnLG6vJGnlWBZhAawHdlfVF6vq/wF3AZctcp8kacVYLmGxGnhm6PWeVpMkTcExi92BMWVErQ46KNkMbG4vX0zy+ER7tbhOAb662J0Y2y0fXOweLCXL67MDPsgti92FpWTZfX6H6btGFZdLWOwBzhx6vQbYO/+gqroduH1anVpMSeaqanax+6HD52e3vK3Uz2+5DEN9BliX5Owkrwc2AtsXuU+StGIsiyuLqnolyb8EPgGsAn6nqnYtcrckacVYFmEBUFUfBz6+2P1YQlbEcNtRys9ueVuRn1+qDponliTpmyyXOQtJ0iIyLBZZkgNJHh5a1rb6Lyb56yTfMXTsxUk+NuIcP5zks0k+l+TRJO9u9RuTfHne+d84rZ9tJUjynUN/tl+Z9+ddbf35JH/86p/9qM8xyR1J3tW272uPtnn1PB9dhB9tRUjy4mEce2OS907q/EvdspmzOIq9VFUXjKhfyeAusB8D7jhU4ySvYzCGur6q9iQ5Flg7dMivV9WvHbHe6ptU1fPABTD4ywR48dU/7yQvvvrZJtkKXAdsGfPUP1lVc0e6v9Jr5ZXFEpTke4ATgH/LIDQW8gYGof88QFW9XFVH85cRl6v78akDy0KSH0nyQLta/9Mkpw3tPj/JnyV5Ism/GGrzr5N8JslfJvnlRej2xBkWi+/4oeGGu1vtSuAjwP8E3pTk1EM1rqqvMfjOyZeSfCTJTyYZ/lx/cej8n5zYT6FDag/CvITD+27Qh4c+t/8woa5ptE8BF1XVWxg8h+59Q/veDPwQ8APAv0tyRpJ3AOsYPMPuAuDCJP9oul2ePIehFt+oYaiNwI9V1d8m+SPgCgZP3R2pqn4myfcDbwfeC/wT4J+33Q5DLZ7jkzzMYFjwIWBHqx/qFsThusNQi2cN8PtJTgdeDzw5tO+eqnoJeKn942s98A+BdwCfbcecwCA8/nx6XZ48ryyWmCRvZvAf2o4kTzEIjt5QFFX1SFX9OoOg+PGJdlLjevUfAt/F4C+d61r9eeCkeceezNH9vKHl5BbgP1XV9wPvBo4b2jc/6IvBs+t+taouaMvfq6rfnlJfp8awWHquBG6sqrVtOQNYnWTkw72SnJDk4qHSBcCXJt5Lja2q/i/w88B72w0JTwBnJPk+gPbZng88vGid1LDvAL7ctjfN23dZkuOSfCdwMYObUD4B/HSSEwCSrF5o6Hi5chhq6dnI4Jc8Dbu71R8ALkmyZ2jflcD7knwQeAn4K74xBAWDOYurhl5fXlVPHelOa2FV9dkknwM2VtXvts/kQ0mOA/4G+JkWKq/6cJKX2vZXq+rt0+7zCvHt8/5/ej9wI/AHSb4MfBo4e2j/g8B/A84CfqWq9gJ7W/DfnwTgReAqYN/kuz89foNbktTlMJQkqcuwkCR1GRaSpC7DQpLUZVhIkroMC+lb5JNLtRIYFpKkLsNCmgCfXKqjjWEhTYZPLtVRxcd9SJPhk0t1VDEspMm4BXh/VW1vD3q8cWjfQk8u/eBUeicdJoehpMnwyaU6qnhlIX3rfHKpjno+dVaS1OUwlCSpy7CQJHUZFpKkLsNCktRlWEiSugwLSVKXYSFJ6jIsJEld/x/sp22G7AHwOAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import DataPrep\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import nltk\n",
    "import nltk.corpus \n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim.models.word2vec import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6ab8d216",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer()\n",
      "  (0, 9676)\t1\n",
      "  (0, 10988)\t1\n",
      "  (0, 1044)\t1\n",
      "  (0, 6639)\t1\n",
      "  (0, 8376)\t1\n",
      "  (0, 5115)\t1\n",
      "  (0, 10709)\t1\n",
      "  (0, 11036)\t1\n",
      "  (0, 11296)\t1\n",
      "  (0, 615)\t1\n",
      "  (0, 7728)\t1\n",
      "  (0, 3278)\t1\n",
      "  (1, 10988)\t1\n",
      "  (1, 11934)\t2\n",
      "  (1, 3434)\t1\n",
      "  (1, 3185)\t1\n",
      "  (1, 7672)\t1\n",
      "  (1, 2475)\t1\n",
      "  (1, 10425)\t1\n",
      "  (1, 6052)\t1\n",
      "  (1, 10426)\t2\n",
      "  (1, 7418)\t1\n",
      "  (1, 4860)\t1\n",
      "  (1, 11138)\t1\n",
      "  (1, 7674)\t1\n",
      "  :\t:\n",
      "  (10239, 10988)\t1\n",
      "  (10239, 7672)\t2\n",
      "  (10239, 11110)\t2\n",
      "  (10239, 5267)\t1\n",
      "  (10239, 7828)\t1\n",
      "  (10239, 7824)\t1\n",
      "  (10239, 1159)\t1\n",
      "  (10239, 12151)\t2\n",
      "  (10239, 6327)\t1\n",
      "  (10239, 6603)\t1\n",
      "  (10239, 11013)\t1\n",
      "  (10239, 11004)\t1\n",
      "  (10239, 3309)\t1\n",
      "  (10239, 12158)\t1\n",
      "  (10239, 11660)\t2\n",
      "  (10239, 799)\t1\n",
      "  (10239, 2568)\t1\n",
      "  (10239, 11622)\t1\n",
      "  (10239, 2549)\t1\n",
      "  (10239, 10660)\t1\n",
      "  (10239, 8996)\t1\n",
      "  (10239, 10918)\t1\n",
      "  (10239, 3989)\t1\n",
      "  (10239, 10594)\t1\n",
      "  (10239, 6853)\t1\n"
     ]
    }
   ],
   "source": [
    "#we will start with simple bag of words technique \n",
    "#creating feature vector - document term matrix\n",
    "countV = CountVectorizer()\n",
    "train_count = countV.fit_transform(DataPrep.train_news['Statement'].values)\n",
    "\n",
    "print(countV)\n",
    "print(train_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2eb7c748",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print training doc term matrix\n",
    "#we have matrix of size of (10240, 12196) by calling below\n",
    "def get_countVectorizer_stats():\n",
    "    \n",
    "    #vocab size\n",
    "    train_count.shape\n",
    "\n",
    "    #check vocabulary using below command\n",
    "    print(countV.vocabulary_)\n",
    "\n",
    "    #get feature names\n",
    "    print(countV.get_feature_names()[:25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "85740c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create tf-df frequency features\n",
    "#tf-idf \n",
    "tfidfV = TfidfTransformer()\n",
    "train_tfidf = tfidfV.fit_transform(train_count)\n",
    "\n",
    "def get_tfidf_stats():\n",
    "    train_tfidf.shape\n",
    "    #get train data feature names \n",
    "    print(train_tfidf.A[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8c0fd68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#bag of words - with n-grams\n",
    "#countV_ngram = CountVectorizer(ngram_range=(1,3),stop_words='english')\n",
    "#tfidf_ngram  = TfidfTransformer(use_idf=True,smooth_idf=True)\n",
    "\n",
    "tfidf_ngram = TfidfVectorizer(stop_words='english',ngram_range=(1,4),use_idf=True,smooth_idf=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b00fe32b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8bfab588",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package treebank to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package treebank is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('treebank')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a4fc34e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        Says the Annies List political group supports ...\n",
      "1        When did the decline of coal start? It started...\n",
      "2        Hillary Clinton agrees with John McCain \"by vo...\n",
      "3        Health care reform legislation is likely to ma...\n",
      "4        The economic turnaround started at the end of ...\n",
      "                               ...                        \n",
      "10235    There are a larger number of shark attacks in ...\n",
      "10236    Democrats have now become the party of the [At...\n",
      "10237    Says an alternative to Social Security that op...\n",
      "10238    On lifting the U.S. Cuban embargo and allowing...\n",
      "10239    The Department of Veterans Affairs has a manua...\n",
      "Name: Statement, Length: 10240, dtype: object\n"
     ]
    }
   ],
   "source": [
    "#POS Tagging\n",
    "tagged_sentences = nltk.corpus.treebank.tagged_sents()\n",
    "\n",
    "cutoff = int(.75 * len(tagged_sentences))\n",
    "training_sentences = DataPrep.train_news['Statement']\n",
    " \n",
    "print(training_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0d741512",
   "metadata": {},
   "outputs": [],
   "source": [
    "#training POS tagger based on words\n",
    "def features(sentence, index):\n",
    "    \"\"\" sentence: [w1, w2, ...], index: the index of the word \"\"\"\n",
    "    return {\n",
    "        'word': sentence[index],\n",
    "        'is_first': index == 0,\n",
    "        'is_last': index == len(sentence) - 1,\n",
    "        'is_capitalized': sentence[index][0].upper() == sentence[index][0],\n",
    "        'is_all_caps': sentence[index].upper() == sentence[index],\n",
    "        'is_all_lower': sentence[index].lower() == sentence[index],\n",
    "        'prefix-1': sentence[index][0],\n",
    "        'prefix-2': sentence[index][:2],\n",
    "        'prefix-3': sentence[index][:3],\n",
    "        'suffix-1': sentence[index][-1],\n",
    "        'suffix-2': sentence[index][-2:],\n",
    "        'suffix-3': sentence[index][-3:],\n",
    "        'prev_word': '' if index == 0 else sentence[index - 1],\n",
    "        'next_word': '' if index == len(sentence) - 1 else sentence[index + 1],\n",
    "        'has_hyphen': '-' in sentence[index],\n",
    "        'is_numeric': sentence[index].isdigit(),\n",
    "        'capitals_inside': sentence[index][1:].lower() != sentence[index][1:]\n",
    "    }\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "989ec89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper function to strip tags from tagged corpus\t\n",
    "def untag(tagged_sentence):\n",
    "    return [w for w, t in tagged_sentence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "26ed5447",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using Word2Vec \n",
    "with open(\"glove.6B.50d.txt\", \"rb\") as lines:\n",
    "    w2v = {line.split()[0]: np.array(map(float, line.split()[1:]))\n",
    "           for line in lines}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ee1e0170",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeanEmbeddingVectorizer(object):\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        # if a text is empty we should return a vector of zeros\n",
    "        # with the same dimensionality as all the other vectors\n",
    "        self.dim = len(word2vec.itervalues().next())\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "            np.mean([self.word2vec[w] for w in words if w in self.word2vec]\n",
    "                    or [np.zeros(self.dim)], axis=0)\n",
    "            for words in X\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a6770cf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nclass TfidfEmbeddingVectorizer(object):\\n    def __init__(self, word2vec):\\n        self.word2vec = word2vec\\n        self.word2weight = None\\n        self.dim = len(word2vec.itervalues().next())\\n\\n    def fit(self, X, y):\\n        tfidf = TfidfVectorizer(analyzer=lambda x: x)\\n        tfidf.fit(X)\\n        # if a word was never seen - it must be at least as infrequent\\n        # as any of the known words - so the default idf is the max of \\n        # known idf's\\n        max_idf = max(tfidf.idf_)\\n        self.word2weight = defaultdict(\\n            lambda: max_idf,\\n            [(w, tfidf.idf_[i]) for w, i in tfidf.vocabulary_.items()])\\n\\n        return self\\n\\n    def transform(self, X):\\n        return np.array([\\n                np.mean([self.word2vec[w] * self.word2weight[w]\\n                         for w in words if w in self.word2vec] or\\n                        [np.zeros(self.dim)], axis=0)\\n                for words in X\\n            ])\\n\\n\""
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "class TfidfEmbeddingVectorizer(object):\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        self.word2weight = None\n",
    "        self.dim = len(word2vec.itervalues().next())\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        tfidf = TfidfVectorizer(analyzer=lambda x: x)\n",
    "        tfidf.fit(X)\n",
    "        # if a word was never seen - it must be at least as infrequent\n",
    "        # as any of the known words - so the default idf is the max of \n",
    "        # known idf's\n",
    "        max_idf = max(tfidf.idf_)\n",
    "        self.word2weight = defaultdict(\n",
    "            lambda: max_idf,\n",
    "            [(w, tfidf.idf_[i]) for w, i in tfidf.vocabulary_.items()])\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "                np.mean([self.word2vec[w] * self.word2weight[w]\n",
    "                         for w in words if w in self.word2vec] or\n",
    "                        [np.zeros(self.dim)], axis=0)\n",
    "                for words in X\n",
    "            ])\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d6f921",
   "metadata": {},
   "source": [
    "### classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e3ddf2e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer()\n",
      "  (0, 9676)\t1\n",
      "  (0, 10988)\t1\n",
      "  (0, 1044)\t1\n",
      "  (0, 6639)\t1\n",
      "  (0, 8376)\t1\n",
      "  (0, 5115)\t1\n",
      "  (0, 10709)\t1\n",
      "  (0, 11036)\t1\n",
      "  (0, 11296)\t1\n",
      "  (0, 615)\t1\n",
      "  (0, 7728)\t1\n",
      "  (0, 3278)\t1\n",
      "  (1, 10988)\t1\n",
      "  (1, 11934)\t2\n",
      "  (1, 3434)\t1\n",
      "  (1, 3185)\t1\n",
      "  (1, 7672)\t1\n",
      "  (1, 2475)\t1\n",
      "  (1, 10425)\t1\n",
      "  (1, 6052)\t1\n",
      "  (1, 10426)\t2\n",
      "  (1, 7418)\t1\n",
      "  (1, 4860)\t1\n",
      "  (1, 11138)\t1\n",
      "  (1, 7674)\t1\n",
      "  :\t:\n",
      "  (10239, 10988)\t1\n",
      "  (10239, 7672)\t2\n",
      "  (10239, 11110)\t2\n",
      "  (10239, 5267)\t1\n",
      "  (10239, 7828)\t1\n",
      "  (10239, 7824)\t1\n",
      "  (10239, 1159)\t1\n",
      "  (10239, 12151)\t2\n",
      "  (10239, 6327)\t1\n",
      "  (10239, 6603)\t1\n",
      "  (10239, 11013)\t1\n",
      "  (10239, 11004)\t1\n",
      "  (10239, 3309)\t1\n",
      "  (10239, 12158)\t1\n",
      "  (10239, 11660)\t2\n",
      "  (10239, 799)\t1\n",
      "  (10239, 2568)\t1\n",
      "  (10239, 11622)\t1\n",
      "  (10239, 2549)\t1\n",
      "  (10239, 10660)\t1\n",
      "  (10239, 8996)\t1\n",
      "  (10239, 10918)\t1\n",
      "  (10239, 3989)\t1\n",
      "  (10239, 10594)\t1\n",
      "  (10239, 6853)\t1\n",
      "0        Says the Annies List political group supports ...\n",
      "1        When did the decline of coal start? It started...\n",
      "2        Hillary Clinton agrees with John McCain \"by vo...\n",
      "3        Health care reform legislation is likely to ma...\n",
      "4        The economic turnaround started at the end of ...\n",
      "                               ...                        \n",
      "10235    There are a larger number of shark attacks in ...\n",
      "10236    Democrats have now become the party of the [At...\n",
      "10237    Says an alternative to Social Security that op...\n",
      "10238    On lifting the U.S. Cuban embargo and allowing...\n",
      "10239    The Department of Veterans Affairs has a manua...\n",
      "Name: Statement, Length: 10240, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import DataPrep\n",
    "import FeatureSelection\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import  LogisticRegression\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import confusion_matrix, f1_score, classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import learning_curve\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import average_precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "eb7bfce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#string to test\n",
    "doc_new = ['obama is running for president in 2016']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c4945673",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6072128577028616"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#the feature selection has been done in FeatureSelection.py module. here we will create models using those features for prediction\n",
    "\n",
    "#first we will use bag of words techniques\n",
    "\n",
    "#building classifier using naive bayes \n",
    "nb_pipeline = Pipeline([\n",
    "        ('NBCV',FeatureSelection.countV),\n",
    "        ('nb_clf',MultinomialNB())])\n",
    "\n",
    "nb_pipeline.fit(DataPrep.train_news['Statement'],DataPrep.train_news['Label'])\n",
    "predicted_nb = nb_pipeline.predict(DataPrep.test_news['Statement'])\n",
    "np.mean(predicted_nb == DataPrep.test_news['Label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "26a1f928",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6013328106624853"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#building classifier using logistic regression\n",
    "logR_pipeline = Pipeline([\n",
    "        ('LogRCV',FeatureSelection.countV),\n",
    "        ('LogR_clf',LogisticRegression())\n",
    "        ])\n",
    "\n",
    "logR_pipeline.fit(DataPrep.train_news['Statement'],DataPrep.train_news['Label'])\n",
    "predicted_LogR = logR_pipeline.predict(DataPrep.test_news['Statement'])\n",
    "np.mean(predicted_LogR == DataPrep.test_news['Label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7a2537b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5723245785966288"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#building Linear SVM classfier\n",
    "svm_pipeline = Pipeline([\n",
    "        ('svmCV',FeatureSelection.countV),\n",
    "        ('svm_clf',svm.LinearSVC())\n",
    "        ])\n",
    "\n",
    "svm_pipeline.fit(DataPrep.train_news['Statement'],DataPrep.train_news['Label'])\n",
    "predicted_svm = svm_pipeline.predict(DataPrep.test_news['Statement'])\n",
    "np.mean(predicted_svm == DataPrep.test_news['Label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2c4defac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:574: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\"Maximum number of iteration reached before \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6138769110152881\n"
     ]
    }
   ],
   "source": [
    "#using SVM Stochastic Gradient Descent on hinge loss\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "sgd_pipeline = Pipeline([\n",
    "        ('svm2CV', CountVectorizer()),\n",
    "        ('svm2_clf', SGDClassifier(loss='hinge', penalty='l2', alpha=1e-3, max_iter=5))\n",
    "])\n",
    "\n",
    "sgd_pipeline.fit(DataPrep.train_news['Statement'], DataPrep.train_news['Label'])\n",
    "predicted_sgd = sgd_pipeline.predict(DataPrep.test_news['Statement'])\n",
    "accuracy = np.mean(predicted_sgd == DataPrep.test_news['Label'])\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8ae10e13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6217169737357899"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#random forest\n",
    "random_forest = Pipeline([\n",
    "        ('rfCV',FeatureSelection.countV),\n",
    "        ('rf_clf',RandomForestClassifier(n_estimators=200,n_jobs=3))\n",
    "        ])\n",
    "    \n",
    "random_forest.fit(DataPrep.train_news['Statement'],DataPrep.train_news['Label'])\n",
    "predicted_rf = random_forest.predict(DataPrep.test_news['Statement'])\n",
    "np.mean(predicted_rf == DataPrep.test_news['Label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "13ff45d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#User defined functon for K-Fold cross validatoin\n",
    "def build_confusion_matrix(classifier):\n",
    "    \n",
    "    k_fold = KFold(n_splits=5)\n",
    "    scores = []\n",
    "    confusion = np.array([[0,0],[0,0]])\n",
    "\n",
    "    for train_ind, test_ind in k_fold.split(DataPrep.train_news):\n",
    "        train_text = DataPrep.train_news.iloc[train_ind]['Statement'] \n",
    "        train_y = DataPrep.train_news.iloc[train_ind]['Label']\n",
    "    \n",
    "        test_text = DataPrep.train_news.iloc[test_ind]['Statement']\n",
    "        test_y = DataPrep.train_news.iloc[test_ind]['Label']\n",
    "        \n",
    "        classifier.fit(train_text,train_y)\n",
    "        predictions = classifier.predict(test_text)\n",
    "        \n",
    "        confusion += confusion_matrix(test_y,predictions)\n",
    "        score = f1_score(test_y,predictions)\n",
    "        scores.append(score)\n",
    "    \n",
    "    return (print('Total statements classified:', len(DataPrep.train_news)),\n",
    "    print('Score:', sum(scores)/len(scores)),\n",
    "    print('score length', len(scores)),\n",
    "    print('Confusion matrix:'),\n",
    "    print(confusion))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "37104959",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total statements classified: 10240\n",
      "Score: 0.66961153965076\n",
      "score length 5\n",
      "Confusion matrix:\n",
      "[[2118 2370]\n",
      " [1664 4088]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total statements classified: 10240\n",
      "Score: 0.6465528860767489\n",
      "score length 5\n",
      "Confusion matrix:\n",
      "[[2254 2234]\n",
      " [1937 3815]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:574: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\"Maximum number of iteration reached before \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total statements classified: 10240\n",
      "Score: 0.6104687487924283\n",
      "score length 5\n",
      "Confusion matrix:\n",
      "[[2260 2228]\n",
      " [2246 3506]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:574: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\"Maximum number of iteration reached before \"\n",
      "C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:574: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\"Maximum number of iteration reached before \"\n",
      "C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:574: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\"Maximum number of iteration reached before \"\n",
      "C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:574: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\"Maximum number of iteration reached before \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total statements classified: 10240\n",
      "Score: 0.6543305148056067\n",
      "score length 5\n",
      "Confusion matrix:\n",
      "[[2175 2313]\n",
      " [1823 3929]]\n",
      "Total statements classified: 10240\n",
      "Score: 0.7004960214362899\n",
      "score length 5\n",
      "Confusion matrix:\n",
      "[[1778 2710]\n",
      " [1190 4562]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None, None, None, None)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#K-fold cross validation for all classifiers\n",
    "build_confusion_matrix(nb_pipeline)\n",
    "build_confusion_matrix(logR_pipeline)\n",
    "build_confusion_matrix(svm_pipeline)\n",
    "build_confusion_matrix(sgd_pipeline)\n",
    "build_confusion_matrix(random_forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7d5be38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#========================================================================================\n",
    "#n-grams & tfidf confusion matrix and F1 scores\n",
    "\n",
    "#Naive bayes\n",
    "# [841 3647]\n",
    "# [427 5325]\n",
    "# f1-Score: 0.723262051071\n",
    "\n",
    "#Logistic regression\n",
    "# [1617 2871]\n",
    "# [1097 4655]\n",
    "# f1-Score: 0.70113000531\n",
    "\n",
    "#svm\n",
    "# [2016 2472]\n",
    "# [1524 4228]\n",
    "# f1-Score: 0.67909201429\n",
    "\n",
    "#sgdclassifier\n",
    "# [  10 4478]\n",
    "# [  13 5739]\n",
    "# f1-Score: 0.718731637053\n",
    "\n",
    "#random forest\n",
    "# [1979 2509]\n",
    "# [1630 4122]\n",
    "# f1-Score: 0.665720333284\n",
    "#========================================================================================="
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
